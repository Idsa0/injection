# -*- coding: utf-8 -*-
"""safeguard.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DBHEYo5rHLbeFKN6fubmdEu1chBeZbnq
"""

#!/usr/bin/env python3
"""
Fine-tuning BERT for Prompt Injection Detection - Dataset 1: Safe-Guard
Uses xTRam1/safe-guard-prompt-injection dataset with 10% holdout for golden test set.
"""

# ================================================================================
# SECTION 1: SETUP AND IMPORTS
# ================================================================================

import os
import argparse
import json
import pickle
import numpy as np
import pandas as pd
import torch
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report
from sklearn.model_selection import train_test_split, StratifiedKFold

from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
    EarlyStoppingCallback,
    set_seed
)
from torch.utils.data import Dataset

os.environ["WANDB_MODE"] = "offline"

set_seed(42)

# ================================================================================
# SECTION 2: ARGUMENT PARSING (COLAB-COMPATIBLE)
# ================================================================================

def parse_arguments():
    """Colab-compatible argument parsing - no argparse needed"""
    class Args:
        def __init__(self):
            self.test_mode = False
            self.output_dir = './safeguard_model'

    return Args()

# alternative version when not using Colab
def get_config(test_mode=True, output_dir='./safeguard_model'):
    """Easy configuration function"""
    class Args:
        def __init__(self):
            self.test_mode = test_mode
            self.output_dir = output_dir

    return Args()

# ================================================================================
# SECTION 3: DATASET CLASS
# ================================================================================

class PromptDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length=512):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts.iloc[idx])
        label = self.labels.iloc[idx]

        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )

        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }

# ================================================================================
# SECTION 4: DATA LOADING AND PREPARATION
# ================================================================================

def load_and_prepare_data(test_mode=False):
    """Load and prepare the Safe-Guard dataset"""

    try:
        dataset = load_dataset("xTRam1/safe-guard-prompt-injection")
    except Exception as e:
        raise Exception(f"Failed to load Safe-Guard dataset: {str(e)}")

    print(f"Available splits: {list(dataset.keys())}")

    all_data = []
    for split_name in dataset.keys():
        split_df = pd.DataFrame(dataset[split_name])
        split_df['split_origin'] = split_name
        all_data.append(split_df)
        print(f"  {split_name}: {len(split_df)} samples")

    # combine all splits
    df = pd.concat(all_data, ignore_index=True)

    print(f"Combined dataset size: {len(df)}")
    print(f"Columns: {df.columns.tolist()}")
    print(f"Label distribution: {df['label'].value_counts().to_dict()}")

    # simple data cleaning
    def clean_text(text):
        if pd.isna(text):
            return ""
        text = str(text).strip()
        return text if len(text) >= 10 else None

    df['text'] = df['text'].apply(clean_text)
    df = df.dropna(subset=['text'])
    df = df[df['text'] != '']

    print(f"After cleaning: {len(df)} samples")

    if len(df) == 0:
        raise Exception("No valid samples found after cleaning")

    # issue warning if needed
    if not test_mode and len(df) > 5000:
        print("WARNING: Training on full dataset may exceed 4-hour Colab limit!")
        print("   Consider using --test_mode for initial testing")

    # reserve 10% of samples for golden test set
    holdout_size = max(1, int(0.1 * len(df)))  # at least 1 sample

    np.random.seed(42)
    golden_indices = np.random.choice(df.index, size=holdout_size, replace=False)

    # save golden test set
    golden_test_df = df.loc[golden_indices].copy()
    golden_test_df.to_csv('golden_test_safeguard.csv', index=False)
    remaining_df = df.drop(golden_indices).copy()

    print(f"Golden test set (10%): {len(golden_test_df)} samples")
    print(f"Training pool (90%): {len(remaining_df)} samples")

    # for test mode use only 5 examples
    if test_mode:
        if len(remaining_df) < 5:
            raise Exception(f"Not enough samples for test mode. Only {len(remaining_df)} available")
        remaining_df = remaining_df.sample(n=5, random_state=42)
        print(f"Test mode: Using only {len(remaining_df)} samples")

    if len(remaining_df) < 2:
        raise Exception("Not enough samples for train/test split")

    try:
        train_df, val_df = train_test_split(
            remaining_df,
            test_size=0.2,
            random_state=42,
            stratify=remaining_df['label']
        )
    except ValueError as e:
        print(f"Stratification failed: {e}. Using regular split.")
        train_df, val_df = train_test_split(
            remaining_df,
            test_size=0.2,
            random_state=42
        )

    print(f"Training set: {len(train_df)} samples")
    print(f"Validation set: {len(val_df)} samples")
    print(f"Training label distribution: {train_df['label'].value_counts().to_dict()}")
    print(f"Validation label distribution: {val_df['label'].value_counts().to_dict()}")

    return train_df, val_df, golden_test_df

# ================================================================================
# SECTION 4.5: MISSING FUNCTIONS
# ================================================================================

def setup_model_and_tokenizer():
    """Setup BERT model and tokenizer"""
    model_name = "distilbert-base-uncased"

    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSequenceClassification.from_pretrained(
        model_name,
        num_labels=2,
        ignore_mismatched_sizes=True
    )

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)

    print(f"Model loaded on device: {device}")
    return model, tokenizer, device

def compute_metrics(eval_pred):
    """Compute evaluation metrics"""
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)

    accuracy = accuracy_score(labels, predictions)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary')

    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1
    }

# ================================================================================
# SECTION 5: MODEL SETUP AND TRAINING
# ================================================================================

def train_model(model, tokenizer, train_df, val_df, output_dir, test_mode=False):
    """Train the model"""

    train_dataset = PromptDataset(train_df['text'], train_df['label'], tokenizer)
    val_dataset = PromptDataset(val_df['text'], val_df['label'], tokenizer)

    training_args = TrainingArguments(
        output_dir=f'{output_dir}/checkpoints',
        num_train_epochs=3 if not test_mode else 1,
        per_device_train_batch_size=8,
        per_device_eval_batch_size=16,
        warmup_steps=500 if not test_mode else 10,
        weight_decay=0.01,
        logging_dir=f'{output_dir}/logs',
        logging_steps=50 if not test_mode else 1,
        eval_strategy="steps",
        eval_steps=100 if not test_mode else 1,
        save_steps=500 if not test_mode else 1,
        save_total_limit=2,
        load_best_model_at_end=True,
        metric_for_best_model="f1",
        greater_is_better=True,
        report_to=None,
        disable_tqdm=False,
        seed=42
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        compute_metrics=compute_metrics,
        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]
    )

    print("Starting training...")
    start_time = datetime.now()

    trainer.train()

    end_time = datetime.now()
    training_time = end_time - start_time
    print(f"Training completed in: {training_time}")

    trainer.save_model(output_dir)
    tokenizer.save_pretrained(output_dir)

    return trainer, training_time

# ================================================================================
# SECTION 6: EVALUATION AND VISUALIZATION
# ================================================================================

def evaluate_model(trainer, val_df, golden_test_df, output_dir, tokenizer):
    """Comprehensive evaluation of the trained model"""

    val_results = trainer.evaluate()

    val_dataset = PromptDataset(val_df['text'], val_df['label'], tokenizer)
    val_predictions = trainer.predict(val_dataset)
    val_pred_labels = np.argmax(val_predictions.predictions, axis=1)
    val_true_labels = val_df['label'].values

    golden_dataset = PromptDataset(golden_test_df['text'], golden_test_df['label'], tokenizer)
    golden_predictions = trainer.predict(golden_dataset)
    golden_pred_labels = np.argmax(golden_predictions.predictions, axis=1)
    golden_true_labels = golden_test_df['label'].values

    results = {
        'validation': calculate_detailed_metrics(val_true_labels, val_pred_labels),
        'golden_test': calculate_detailed_metrics(golden_true_labels, golden_pred_labels)
    }

    print_results(results)

    create_visualizations(val_true_labels, val_pred_labels,
                         golden_true_labels, golden_pred_labels, output_dir)

    save_results(results, output_dir)

    return results

def calculate_detailed_metrics(y_true, y_pred):
    """Calculate detailed metrics"""
    accuracy = accuracy_score(y_true, y_pred)
    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary', zero_division=0)
    cm = confusion_matrix(y_true, y_pred)

    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'confusion_matrix': cm,
        'classification_report': classification_report(y_true, y_pred,
                                                     target_names=['Safe', 'Injection'], zero_division=0)
    }

def print_results(results):
    """Print evaluation results"""
    for dataset_name, metrics in results.items():
        print(f"\n{dataset_name.upper()} SET RESULTS:")
        print("-" * 40)
        print(f"Accuracy:  {metrics['accuracy']:.4f}")
        print(f"Precision: {metrics['precision']:.4f}")
        print(f"Recall:    {metrics['recall']:.4f}")
        print(f"F1-Score:  {metrics['f1']:.4f}")
        print(f"\nClassification Report:\n{metrics['classification_report']}")

def create_visualizations(val_true, val_pred, golden_true, golden_pred, output_dir):
    """Create and save visualizations"""

    fig, axes = plt.subplots(1, 2, figsize=(15, 6))

    cm_val = confusion_matrix(val_true, val_pred)
    sns.heatmap(cm_val, annot=True, fmt='d', cmap='Blues',
               xticklabels=['Safe', 'Injection'],
               yticklabels=['Safe', 'Injection'], ax=axes[0])
    axes[0].set_title('Validation Set - Confusion Matrix')
    axes[0].set_ylabel('True Label')
    axes[0].set_xlabel('Predicted Label')

    cm_golden = confusion_matrix(golden_true, golden_pred)
    sns.heatmap(cm_golden, annot=True, fmt='d', cmap='Blues',
               xticklabels=['Safe', 'Injection'],
               yticklabels=['Safe', 'Injection'], ax=axes[1])
    axes[1].set_title('Golden Test Set - Confusion Matrix')
    axes[1].set_ylabel('True Label')
    axes[1].set_xlabel('Predicted Label')

    plt.tight_layout()
    plt.savefig(f'{output_dir}/confusion_matrices.png', dpi=300, bbox_inches='tight')
    plt.show()

    metrics_data = {
        'Validation': [
            accuracy_score(val_true, val_pred),
            precision_recall_fscore_support(val_true, val_pred, average='binary', zero_division=0)[0],
            precision_recall_fscore_support(val_true, val_pred, average='binary', zero_division=0)[1],
            precision_recall_fscore_support(val_true, val_pred, average='binary', zero_division=0)[2]
        ],
        'Golden Test': [
            accuracy_score(golden_true, golden_pred),
            precision_recall_fscore_support(golden_true, golden_pred, average='binary', zero_division=0)[0],
            precision_recall_fscore_support(golden_true, golden_pred, average='binary', zero_division=0)[1],
            precision_recall_fscore_support(golden_true, golden_pred, average='binary', zero_division=0)[2]
        ]
    }

    metrics_df = pd.DataFrame(metrics_data, index=['Accuracy', 'Precision', 'Recall', 'F1-Score'])

    plt.figure(figsize=(10, 6))
    metrics_df.plot(kind='bar', width=0.8)
    plt.title('Model Performance Comparison: Safe-Guard Dataset')
    plt.ylabel('Score')
    plt.xlabel('Metrics')
    plt.legend(title='Dataset')
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.savefig(f'{output_dir}/metrics_comparison.png', dpi=300, bbox_inches='tight')
    plt.show()

def save_results(results, output_dir):
    """Save results to files"""

    results_json = {}
    for dataset_name, metrics in results.items():
        results_json[dataset_name] = {
            'accuracy': float(metrics['accuracy']),
            'precision': float(metrics['precision']),
            'recall': float(metrics['recall']),
            'f1': float(metrics['f1']),
            'confusion_matrix': metrics['confusion_matrix'].tolist()
        }

    with open(f'{output_dir}/results.json', 'w') as f:
        json.dump(results_json, f, indent=4)

    with open(f'{output_dir}/results.pkl', 'wb') as f:
        pickle.dump(results, f)

    print(f"\nResults saved to {output_dir}/")
    print("Files created:")
    print("- results.json (metrics)")
    print("- results.pkl (full results)")
    print("- confusion_matrices.png")
    print("- metrics_comparison.png")
    print("- pytorch_model.bin (trained model)")
    print("- config.json (model config)")
    print("- tokenizer files")

# ================================================================================
# SECTION 7: MAIN EXECUTION
# ================================================================================

def main():
    args = parse_arguments()

    print(f"Running in {'TEST' if args.test_mode else 'FULL'} mode")
    print(f"Output directory: {args.output_dir}")

    os.makedirs(args.output_dir, exist_ok=True)

    try:
        train_df, val_df, golden_test_df = load_and_prepare_data(args.test_mode)
        model, tokenizer, device = setup_model_and_tokenizer()
        trainer, training_time = train_model(model, tokenizer, train_df, val_df,
                                           args.output_dir, args.test_mode)
        results = evaluate_model(trainer, val_df, golden_test_df, args.output_dir, tokenizer)

        print(f"Dataset: Safe-Guard Prompt Injection")
        print(f"Training time: {training_time}")
        print(f"Best F1-Score (validation): {results['validation']['f1']:.4f}")
        print(f"Golden test F1-Score: {results['golden_test']['f1']:.4f}")
        print(f"Model saved to: {args.output_dir}")
        print("Golden test set saved as: golden_test_safeguard.csv")

    except Exception as e:
        print(f"ERROR: {str(e)}")
        import traceback
        traceback.print_exc()
        raise e

if __name__ == "__main__":
    main()

# ================================================================================
# SECTION 8: DOWNLOAD FILES FROM COLAB
# ================================================================================

import os
import zipfile
from google.colab import files

print("Checking main safeguard_model directory:")
if os.path.exists('./safeguard_model'):
    for f in os.listdir('./safeguard_model'):
        if os.path.isfile(os.path.join('./safeguard_model', f)):
            size = os.path.getsize(os.path.join('./safeguard_model', f))
            print(f"  {f} ({size:,} bytes)")

print("\nChecking checkpoint directory:")
checkpoint_dir = './safeguard_model/checkpoints/checkpoint-500'
if os.path.exists(checkpoint_dir):
    for f in os.listdir(checkpoint_dir):
        size = os.path.getsize(os.path.join(checkpoint_dir, f))
        print(f"  {f} ({size:,} bytes)")

def download_safeguard_complete():
    zip_filename = 'safeguard_model_complete.zip'

    with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:
        main_dir = './safeguard_model'
        if os.path.exists(main_dir):
            for f in os.listdir(main_dir):
                if os.path.isfile(os.path.join(main_dir, f)):
                    file_path = os.path.join(main_dir, f)

                    if f.endswith(('.safetensors', '.json', '.txt')) and 'results' not in f:
                        zipf.write(file_path, f'model/{f}')
                    else:
                        zipf.write(file_path, f'results/{f}')

                    size = os.path.getsize(file_path)
                    print(f"Added: {f} ({size/1024/1024:.1f}MB)")

        if os.path.exists('golden_test_safeguard.csv'):
            zipf.write('golden_test_safeguard.csv', 'data/golden_test_safeguard.csv')
            print("Added: golden_test_safeguard.csv")

    print(f"Package created: {zip_filename}")
    files.download(zip_filename)

download_safeguard_complete()
