# -*- coding: utf-8 -*-
"""combined.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jMKZVW2EM2xXeclTOrBKZRYlQz74GaGA
"""

# -*- coding: utf-8 -*-
"""
Fine-tuning BERT for Prompt Injection Detection - Combined Dataset
Uses hackaprompt/hackaprompt-dataset (10k samples) + HuggingFaceH4/helpful-instructions (10k samples)
Total: 20k samples with 10% holdout for golden test set.
"""

# ================================================================================
# SECTION 1: SETUP AND IMPORTS
# ================================================================================

import os
import argparse
import json
import pickle
import numpy as np
import pandas as pd
import torch
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report
from sklearn.model_selection import train_test_split, StratifiedKFold

from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
    EarlyStoppingCallback,
    set_seed
)
from torch.utils.data import Dataset

os.environ["WANDB_MODE"] = "offline"

set_seed(42)

# ================================================================================
# SECTION 2: ARGUMENT PARSING (COLAB-COMPATIBLE)
# ================================================================================

def parse_arguments():
    """Colab-compatible argument parsing - no argparse needed"""
    class Args:
        def __init__(self):
            self.test_mode = False
            self.output_dir = './combined_model'

    return Args()

# alternative version with easy configuration
def get_config(test_mode=True, output_dir='./combined_model'):
    """Easy configuration function"""
    class Args:
        def __init__(self):
            self.test_mode = test_mode
            self.output_dir = output_dir

    return Args()

# ================================================================================
# SECTION 3: DATASET CLASS
# ================================================================================

class PromptDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length=512):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts.iloc[idx])
        label = self.labels.iloc[idx]

        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )

        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }

# ================================================================================
# SECTION 4: DATA LOADING AND PREPARATION - MODIFIED FOR COMBINED DATASET
# ================================================================================

def load_hackaprompt_data(n_samples=10000):
    """Load and prepare hackaprompt dataset (injection examples)"""

    try:
        dataset = load_dataset("hackaprompt/hackaprompt-dataset")
    except Exception as e:
        raise Exception(f"Failed to load hackaprompt dataset: {str(e)}")

    print(f"Available splits: {list(dataset.keys())}")

    all_data = []
    for split_name in dataset.keys():
        split_df = pd.DataFrame(dataset[split_name])
        all_data.append(split_df)
        print(f"  {split_name}: {len(split_df)} samples")

    # combine all splits
    df = pd.concat(all_data, ignore_index=True)
    print(f"Total hackaprompt samples: {len(df)}")
    print(f"Columns: {df.columns.tolist()}")

    text_columns = ['prompt', 'text', 'user_input', 'input', 'question']
    text_column = None

    for col in text_columns:
        if col in df.columns:
            text_column = col
            break

    if text_column is None:
        print(f"Warning: Standard text columns not found. Available columns: {df.columns.tolist()}")
        print("Using first column as text column")
        text_column = df.columns[0]

    print(f"Using '{text_column}' as text column")

    df_clean = df[[text_column]].copy()
    df_clean.columns = ['text']
    df_clean['label'] = 1
    df_clean['source'] = 'hackaprompt'

    def clean_text(text):
        if pd.isna(text):
            return ""
        text = str(text).strip()
        return text if len(text) >= 10 else None

    df_clean['text'] = df_clean['text'].apply(clean_text)
    df_clean = df_clean.dropna(subset=['text'])
    df_clean = df_clean[df_clean['text'] != '']

    print(f"After cleaning: {len(df_clean)} samples")

    if len(df_clean) > n_samples:
        df_clean = df_clean.sample(n=n_samples, random_state=42)
        print(f"Randomly sampled {n_samples} examples")
    elif len(df_clean) < n_samples:
        print(f"Warning: Only {len(df_clean)} samples available, less than requested {n_samples}")

    return df_clean

def load_helpful_instructions_data(n_samples=10000):
    """Load and prepare helpful instructions dataset (safe examples)"""

    try:
        dataset = load_dataset("HuggingFaceH4/helpful-instructions")
    except Exception as e:
        raise Exception(f"Failed to load helpful-instructions dataset: {str(e)}")

    print(f"Available splits: {list(dataset.keys())}")

    all_data = []
    for split_name in dataset.keys():
        split_df = pd.DataFrame(dataset[split_name])
        all_data.append(split_df)
        print(f"  {split_name}: {len(split_df)} samples")

    # combine all splits
    df = pd.concat(all_data, ignore_index=True)
    print(f"Total helpful-instructions samples: {len(df)}")
    print(f"Columns: {df.columns.tolist()}")

    text_columns = ['instruction', 'prompt', 'text', 'input', 'question']
    text_column = None

    for col in text_columns:
        if col in df.columns:
            text_column = col
            break

    if text_column is None:
        print(f"Warning: Standard text columns not found. Available columns: {df.columns.tolist()}")
        print("Using first column as text column")
        text_column = df.columns[0]

    print(f"Using '{text_column}' as text column")

    df_clean = df[[text_column]].copy()
    df_clean.columns = ['text']
    df_clean['label'] = 0
    df_clean['source'] = 'helpful-instructions'

    def clean_text(text):
        if pd.isna(text):
            return ""
        text = str(text).strip()
        return text if len(text) >= 10 else None

    df_clean['text'] = df_clean['text'].apply(clean_text)
    df_clean = df_clean.dropna(subset=['text'])
    df_clean = df_clean[df_clean['text'] != '']

    print(f"After cleaning: {len(df_clean)} samples")

    if len(df_clean) > n_samples:
        df_clean = df_clean.sample(n=n_samples, random_state=42)
        print(f"Randomly sampled {n_samples} examples")
    elif len(df_clean) < n_samples:
        print(f"Warning: Only {len(df_clean)} samples available, less than requested {n_samples}")

    return df_clean

def load_and_prepare_data(test_mode=False):
    """Load and prepare the combined dataset"""

    # sample size
    if test_mode:
        hack_samples = 5
        helpful_samples = 5
    else:
        hack_samples = 10000
        helpful_samples = 10000

    hack_df = load_hackaprompt_data(hack_samples)
    helpful_df = load_helpful_instructions_data(helpful_samples)
    df = pd.concat([hack_df, helpful_df], ignore_index=True)

    print(f"Total samples: {len(df)}")
    print(f"Label distribution: {df['label'].value_counts().to_dict()}")
    print(f"Source distribution: {df['source'].value_counts().to_dict()}")

    if len(df) == 0:
        raise Exception("No valid samples found after combining datasets")

    # print(f"\nSAMPLE TEXTS:")
    # print("=" * 50)
    # print("INJECTION EXAMPLES:")
    # injection_samples = df[df['label'] == 1].head(2)
    # for idx, row in injection_samples.iterrows():
    #     print(f"  {row['text'][:100]}...")

    # print("\nSAFE EXAMPLES:")
    # safe_samples = df[df['label'] == 0].head(2)
    # for idx, row in safe_samples.iterrows():
    #     print(f"  {row['text'][:100]}...")

    # issue warning if needed
    if not test_mode and len(df) > 5000:
        print("\nWARNING: Training on 20k dataset may exceed 4-hour Colab limit!")
        print("   Consider using --test_mode for initial testing")

    # reserve 10% of samples for golden test set
    holdout_size = max(1, int(0.1 * len(df)))  # at least 1 sample

    np.random.seed(42)
    golden_indices = np.random.choice(df.index, size=holdout_size, replace=False)

    # save golden test set
    golden_test_df = df.loc[golden_indices].copy()
    golden_test_df.to_csv('golden_test_combined.csv', index=False)
    remaining_df = df.drop(golden_indices).copy()

    print(f"\nGolden test set (10%): {len(golden_test_df)} samples")
    print(f"Golden test labels: {golden_test_df['label'].value_counts().to_dict()}")
    print(f"Training pool (90%): {len(remaining_df)} samples")
    print(f"Training pool labels: {remaining_df['label'].value_counts().to_dict()}")

    if len(remaining_df) < 2:
        raise Exception("Not enough samples for train/test split")

    try:
        train_df, val_df = train_test_split(
            remaining_df,
            test_size=0.2,
            random_state=42,
            stratify=remaining_df['label']
        )
    except ValueError as e:
        print(f"Stratification failed: {e}. Using regular split.")
        train_df, val_df = train_test_split(
            remaining_df,
            test_size=0.2,
            random_state=42
        )

    print(f"Training set: {len(train_df)} samples")
    print(f"  - Safe: {(train_df['label'] == 0).sum()}")
    print(f"  - Injection: {(train_df['label'] == 1).sum()}")
    print(f"Validation set: {len(val_df)} samples")
    print(f"  - Safe: {(val_df['label'] == 0).sum()}")
    print(f"  - Injection: {(val_df['label'] == 1).sum()}")

    return train_df, val_df, golden_test_df

# ================================================================================
# SECTION 4.5: MISSING FUNCTIONS
# ================================================================================

def setup_model_and_tokenizer():
    """Setup BERT model and tokenizer"""
    model_name = "distilbert-base-uncased"

    print(f"Loading model: {model_name}")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSequenceClassification.from_pretrained(
        model_name,
        num_labels=2,
        ignore_mismatched_sizes=True
    )

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)

    print(f"Model loaded on device: {device}")
    return model, tokenizer, device

def compute_metrics(eval_pred):
    """Compute evaluation metrics"""
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)

    accuracy = accuracy_score(labels, predictions)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary')

    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1
    }

# ================================================================================
# SECTION 5: MODEL SETUP AND TRAINING
# ================================================================================

def train_model(model, tokenizer, train_df, val_df, output_dir, test_mode=False):
    """Train the model"""

    train_dataset = PromptDataset(train_df['text'], train_df['label'], tokenizer)
    val_dataset = PromptDataset(val_df['text'], val_df['label'], tokenizer)

    training_args = TrainingArguments(
        output_dir=f'{output_dir}/checkpoints',
        num_train_epochs=3 if not test_mode else 1,
        per_device_train_batch_size=8,
        per_device_eval_batch_size=16,
        warmup_steps=500 if not test_mode else 10,
        weight_decay=0.01,
        logging_dir=f'{output_dir}/logs',
        logging_steps=50 if not test_mode else 1,
        eval_strategy="steps",
        eval_steps=100 if not test_mode else 1,
        save_steps=500 if not test_mode else 1,
        save_total_limit=2,
        load_best_model_at_end=True,
        metric_for_best_model="f1",
        greater_is_better=True,
        report_to=None,
        disable_tqdm=False,
        seed=42
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        compute_metrics=compute_metrics,
        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]
    )

    print("Starting training...")
    start_time = datetime.now()

    trainer.train()

    end_time = datetime.now()
    training_time = end_time - start_time
    print(f"Training completed in: {training_time}")

    trainer.save_model(output_dir)
    tokenizer.save_pretrained(output_dir)

    return trainer, training_time

# ================================================================================
# SECTION 6: EVALUATION AND VISUALIZATION
# ================================================================================

def evaluate_model(trainer, val_df, golden_test_df, output_dir, tokenizer):
    """Comprehensive evaluation of the trained model"""

    val_results = trainer.evaluate()

    val_dataset = PromptDataset(val_df['text'], val_df['label'], tokenizer)
    val_predictions = trainer.predict(val_dataset)
    val_pred_labels = np.argmax(val_predictions.predictions, axis=1)
    val_true_labels = val_df['label'].values

    golden_dataset = PromptDataset(golden_test_df['text'], golden_test_df['label'], tokenizer)
    golden_predictions = trainer.predict(golden_dataset)
    golden_pred_labels = np.argmax(golden_predictions.predictions, axis=1)
    golden_true_labels = golden_test_df['label'].values

    results = {
        'validation': calculate_detailed_metrics(val_true_labels, val_pred_labels),
        'golden_test': calculate_detailed_metrics(golden_true_labels, golden_pred_labels)
    }

    print_results(results)

    create_visualizations(val_true_labels, val_pred_labels,
                         golden_true_labels, golden_pred_labels, output_dir)

    save_results(results, output_dir)

    return results

def calculate_detailed_metrics(y_true, y_pred):
    """Calculate detailed metrics"""
    accuracy = accuracy_score(y_true, y_pred)
    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary', zero_division=0)
    cm = confusion_matrix(y_true, y_pred)

    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'confusion_matrix': cm,
        'classification_report': classification_report(y_true, y_pred,
                                                     target_names=['Safe', 'Injection'], zero_division=0)
    }

def print_results(results):
    """Print evaluation results"""
    for dataset_name, metrics in results.items():
        print(f"\n{dataset_name.upper()} SET RESULTS:\n")
        print(f"Accuracy:  {metrics['accuracy']:.4f}")
        print(f"Precision: {metrics['precision']:.4f}")
        print(f"Recall:    {metrics['recall']:.4f}")
        print(f"F1-Score:  {metrics['f1']:.4f}")
        print(f"\nClassification Report:\n{metrics['classification_report']}")

def create_visualizations(val_true, val_pred, golden_true, golden_pred, output_dir):
    """Create and save visualizations"""

    fig, axes = plt.subplots(1, 2, figsize=(15, 6))

    cm_val = confusion_matrix(val_true, val_pred)
    sns.heatmap(cm_val, annot=True, fmt='d', cmap='Blues',
               xticklabels=['Safe', 'Injection'],
               yticklabels=['Safe', 'Injection'], ax=axes[0])
    axes[0].set_title('Validation Set - Confusion Matrix')
    axes[0].set_ylabel('True Label')
    axes[0].set_xlabel('Predicted Label')

    cm_golden = confusion_matrix(golden_true, golden_pred)
    sns.heatmap(cm_golden, annot=True, fmt='d', cmap='Blues',
               xticklabels=['Safe', 'Injection'],
               yticklabels=['Safe', 'Injection'], ax=axes[1])
    axes[1].set_title('Golden Test Set - Confusion Matrix')
    axes[1].set_ylabel('True Label')
    axes[1].set_xlabel('Predicted Label')

    plt.tight_layout()
    plt.savefig(f'{output_dir}/confusion_matrices.png', dpi=300, bbox_inches='tight')
    plt.show()

    metrics_data = {
        'Validation': [
            accuracy_score(val_true, val_pred),
            precision_recall_fscore_support(val_true, val_pred, average='binary', zero_division=0)[0],
            precision_recall_fscore_support(val_true, val_pred, average='binary', zero_division=0)[1],
            precision_recall_fscore_support(val_true, val_pred, average='binary', zero_division=0)[2]
        ],
        'Golden Test': [
            accuracy_score(golden_true, golden_pred),
            precision_recall_fscore_support(golden_true, golden_pred, average='binary', zero_division=0)[0],
            precision_recall_fscore_support(golden_true, golden_pred, average='binary', zero_division=0)[1],
            precision_recall_fscore_support(golden_true, golden_pred, average='binary', zero_division=0)[2]
        ]
    }

    metrics_df = pd.DataFrame(metrics_data, index=['Accuracy', 'Precision', 'Recall', 'F1-Score'])

    plt.figure(figsize=(10, 6))
    metrics_df.plot(kind='bar', width=0.8)
    plt.title('Model Performance Comparison: Combined Dataset (HackAPrompt + Helpful Instructions)')
    plt.ylabel('Score')
    plt.xlabel('Metrics')
    plt.legend(title='Dataset')
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.savefig(f'{output_dir}/metrics_comparison.png', dpi=300, bbox_inches='tight')
    plt.show()

def save_results(results, output_dir):
    """Save results to files"""

    results_json = {}
    for dataset_name, metrics in results.items():
        results_json[dataset_name] = {
            'accuracy': float(metrics['accuracy']),
            'precision': float(metrics['precision']),
            'recall': float(metrics['recall']),
            'f1': float(metrics['f1']),
            'confusion_matrix': metrics['confusion_matrix'].tolist()
        }

    with open(f'{output_dir}/results.json', 'w') as f:
        json.dump(results_json, f, indent=4)

    with open(f'{output_dir}/results.pkl', 'wb') as f:
        pickle.dump(results, f)

    print(f"\nResults saved to {output_dir}/")
    print("Files created:")
    print("- results.json (metrics)")
    print("- results.pkl (full results)")
    print("- confusion_matrices.png")
    print("- metrics_comparison.png")
    print("- pytorch_model.bin (trained model)")
    print("- config.json (model config)")
    print("- tokenizer files")

# ================================================================================
# SECTION 7: MAIN EXECUTION
# ================================================================================

def main():
    args = parse_arguments()

    print(f"Running in {'TEST' if args.test_mode else 'FULL'} mode")
    print(f"Output directory: {args.output_dir}")

    os.makedirs(args.output_dir, exist_ok=True)

    try:
        train_df, val_df, golden_test_df = load_and_prepare_data(args.test_mode)
        model, tokenizer, device = setup_model_and_tokenizer()
        trainer, training_time = train_model(model, tokenizer, train_df, val_df,
                                           args.output_dir, args.test_mode)
        results = evaluate_model(trainer, val_df, golden_test_df, args.output_dir, tokenizer)

        print(f"Dataset: Combined (HackAPrompt + Helpful Instructions)")
        print(f"Total samples: {len(train_df) + len(val_df) + len(golden_test_df)}")
        print(f"Training time: {training_time}")
        print(f"Best F1-Score (validation): {results['validation']['f1']:.4f}")
        print(f"Golden test F1-Score: {results['golden_test']['f1']:.4f}")
        print(f"Model saved to: {args.output_dir}")
        print("Golden test set saved as: golden_test_combined.csv")

    except Exception as e:
        print(f"ERROR: {str(e)}")
        import traceback
        traceback.print_exc()
        raise e

if __name__ == "__main__":
    main()

# ================================================================================
# SECTION 8: DOWNLOAD FILES FROM COLAB
# ================================================================================

import zipfile
import os
from google.colab import files

def download_complete_model():
    """Download your trained model with all files"""

    output_dir = './combined_model'
    zip_filename = 'combined_model_complete.zip'

    with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:
        files_to_package = [
            'model.safetensors',
            'config.json',
            'vocab.txt',
            'tokenizer.json',
            'tokenizer_config.json',
            'special_tokens_map.json',
            'results.json',
            'results.pkl',
            'confusion_matrices.png',
            'metrics_comparison.png'
        ]

        print("Packaging files:")
        for file in files_to_package:
            file_path = os.path.join(output_dir, file)
            if os.path.exists(file_path):
                if file.endswith(('.safetensors', '.json', '.txt')) and file != 'results.json':
                    zipf.write(file_path, f'model/{file}')
                else:
                    zipf.write(file_path, f'results/{file}')

                size = os.path.getsize(file_path)
                print(f"  {file} ({size/1024/1024:.1f}MB)")

        if os.path.exists('golden_test_combined.csv'):
            zipf.write('golden_test_combined.csv', 'data/golden_test_combined.csv')
            print("  golden_test_combined.csv")

    print(f"\nPackage created: {zip_filename}")
    zip_size = os.path.getsize(zip_filename)
    print(f"Total size: {zip_size/1024/1024:.1f}MB")

    files.download(zip_filename)
    print("Download started!")

download_complete_model()
