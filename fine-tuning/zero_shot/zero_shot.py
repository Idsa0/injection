# -*- coding: utf-8 -*-
"""zero-shot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kNcNsmZalBtGK5S9DeZfRpqb9mX8S8yY
"""

# ================================================================================
# SECTION 1: SETUP AND IMPORTS
# ================================================================================

import os
os.environ["WANDB_MODE"] = "offline"

from datasets import load_dataset
import pandas as pd
import numpy as np
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from torch.utils.data import Dataset, DataLoader
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm

torch.manual_seed(42)
np.random.seed(42)

print("All imports successful!")
print(f"PyTorch version: {torch.__version__}")
print(f"Using device: {'CUDA' if torch.cuda.is_available() else 'CPU'}")

# ================================================================================
# SECTION 2: HELPER FUNCTIONS AND CLASSES
# ================================================================================

def clean_text(text):
    """Clean text data"""
    if pd.isna(text):
        return ""
    text = str(text).strip()
    return text if len(text) >= 10 else None

class SimpleDataset(Dataset):
    """Dataset class for prompt injection data"""
    def __init__(self, texts, labels, tokenizer, max_length=512):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts.iloc[idx])
        label = self.labels.iloc[idx]

        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )

        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long),
            'text': text
        }

print("Helper functions and classes defined!")

# ================================================================================
# SECTION 3: DATA LOADING AND PREPARATION
# ================================================================================

dataset = load_dataset("xTRam1/safe-guard-prompt-injection")
df = pd.DataFrame(dataset['train'])

print(f"Loaded {len(df)} samples")
print(f"Columns: {df.columns.tolist()}")
print(f"Original label distribution:")
print(df['label'].value_counts())

df['text'] = df['text'].apply(clean_text)
df = df.dropna(subset=['text'])
df = df[df['text'] != '']

print(f"After cleaning: {len(df)} samples")
print(f"Final label distribution:")
print(df['label'].value_counts())

# ================================================================================
# SECTION 4: CREATE TEST SAMPLE AND ANALYZE
# ================================================================================

sample_size = min(1000, len(df))
df_sample = df.sample(n=sample_size, random_state=42)

print(f"Label distribution in test set:")
label_counts = df_sample['label'].value_counts().sort_index()
for label, count in label_counts.items():
    label_name = "Safe" if label == 0 else "Injection"
    percentage = (count / len(df_sample)) * 100
    print(f"  {label_name} (label {label}): {count} samples ({percentage:.1f}%)")

text_lengths = df_sample['text'].str.len()
print(f"\nText length statistics:")
print(f"  Average length: {text_lengths.mean():.1f} characters")
print(f"  Median length: {text_lengths.median():.1f} characters")
print(f"  Min length: {text_lengths.min()} characters")
print(f"  Max length: {text_lengths.max()} characters")

# ================================================================================
# SECTION 5: LOAD PRE-TRAINED MODEL
# ================================================================================

def load_pretrained_model(model_name="distilbert-base-uncased"):
    """Load pre-trained model without any training"""
    print(f"Loading pre-trained model: {model_name}")

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")

    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSequenceClassification.from_pretrained(
        model_name,
        num_labels=2,
        ignore_mismatched_sizes=True
    )

    model.to(device)
    model.eval()

    return model, tokenizer, device

model, tokenizer, device = load_pretrained_model()
print("Model loaded successfully!")

# ================================================================================
# SECTION 6: PREPARE DATA FOR TESTING
# ================================================================================

dataset = SimpleDataset(df_sample['text'], df_sample['label'], tokenizer)
dataloader = DataLoader(dataset, batch_size=16, shuffle=False)

print(f"Dataset created with {len(dataset)} samples")
print(f"DataLoader created with batch size 16")

# ================================================================================
# SECTION 7: MAKE PREDICTIONS
# ================================================================================

def predict_batch(model, dataloader, device):
    """Make predictions on a batch of data"""
    all_predictions = []
    all_labels = []
    all_texts = []
    all_probabilities = []

    model.eval()
    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Making predictions"):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            logits = outputs.logits

            probabilities = torch.softmax(logits, dim=-1)
            predictions = torch.argmax(logits, dim=-1)

            all_predictions.extend(predictions.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            all_texts.extend(batch['text'])
            all_probabilities.extend(probabilities.cpu().numpy())

    return all_predictions, all_labels, all_texts, all_probabilities

y_pred, y_true, texts, probabilities = predict_batch(model, dataloader, device)
print("Predictions completed!")

# ================================================================================
# SECTION 8: ANALYZE BASIC RESULTS
# ================================================================================

def analyze_results(y_true, y_pred, texts, probabilities):
    """Analyze and visualize results"""

    accuracy = accuracy_score(y_true, y_pred)
    print(f"Overall Accuracy: {accuracy:.2%}")

    print("\nClassification Report:")
    print(classification_report(y_true, y_pred, target_names=['Safe', 'Injection']))

    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
               xticklabels=['Safe', 'Injection'],
               yticklabels=['Safe', 'Injection'])
    plt.title('Confusion Matrix - Zero-Shot Classification')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.show()

    tn, fp, fn, tp = cm.ravel()
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0

    print(f"\nDetailed Metrics:")
    print(f"True Negatives:  {tn}")
    print(f"False Positives: {fp}")
    print(f"False Negatives: {fn}")
    print(f"True Positives:  {tp}")
    print(f"Precision: {precision:.2%}")
    print(f"Recall: {recall:.2%}")
    print(f"F1-Score: {f1:.2%}")

    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'confusion_matrix': cm
    }

results = analyze_results(y_true, y_pred, texts, probabilities)

# ================================================================================
# SECTION 9: CONFIDENCE ANALYSIS
# ================================================================================

def analyze_confidence_distribution(y_true, y_pred, probabilities):
    """Analyze the confidence distribution of predictions"""
    print("\n" + "="*60)
    print("CONFIDENCE ANALYSIS")
    print("="*60)

    confidences = np.max(probabilities, axis=1)

    correct_mask = (np.array(y_true) == np.array(y_pred))
    correct_confidences = confidences[correct_mask]
    incorrect_confidences = confidences[~correct_mask]

    print(f"Average confidence for correct predictions: {np.mean(correct_confidences):.3f}")
    print(f"Average confidence for incorrect predictions: {np.mean(incorrect_confidences):.3f}")

    plt.figure(figsize=(12, 5))

    plt.subplot(1, 2, 1)
    plt.hist(correct_confidences, bins=20, alpha=0.7, label='Correct', color='green')
    plt.hist(incorrect_confidences, bins=20, alpha=0.7, label='Incorrect', color='red')
    plt.xlabel('Confidence')
    plt.ylabel('Frequency')
    plt.title('Confidence Distribution')
    plt.legend()

    plt.subplot(1, 2, 2)
    thresholds = np.linspace(0.5, 1.0, 20)
    accuracies = []
    sample_counts = []

    for threshold in thresholds:
        high_conf_mask = confidences >= threshold
        if np.sum(high_conf_mask) > 0:
            high_conf_accuracy = np.mean(np.array(y_true)[high_conf_mask] == np.array(y_pred)[high_conf_mask])
            accuracies.append(high_conf_accuracy)
            sample_counts.append(np.sum(high_conf_mask))
        else:
            accuracies.append(0)
            sample_counts.append(0)

    plt.plot(thresholds, accuracies, 'b-o', label='Accuracy')
    plt.xlabel('Confidence Threshold')
    plt.ylabel('Accuracy')
    plt.title('Accuracy vs Confidence Threshold')
    plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

    return confidences

confidences = analyze_confidence_distribution(y_true, y_pred, probabilities)

# ================================================================================
# SECTION 10: FINAL SUMMARY
# ================================================================================

print(f"Zero-shot accuracy: {results['accuracy']:.2%}")
print(f"Precision: {results['precision']:.2%}")
print(f"Recall: {results['recall']:.2%}")
print(f"F1-Score: {results['f1']:.2%}")
